<p>This document is a systematic comparison of <a href="/riak/kv/2.2.3/using/reference/v2-multi-datacenter">Version 2</a> and <a href="/riak/kv/2.2.3/using/reference/v3-multi-datacenter">Version 3</a> of Riak Enterprise’s Multi-Datacenter
Replication capabilities.</p>

<p>If you are installing Riak Enterprise anew, you should use version 3
replication. Under no circumstances should you mix version 2 and version 3
replication. This comparison is meant only to list improvements introduced in
version 3.</p>

<h2 id="version-2">Version 2</h2>

<ul>
  <li>Version 2 replication relies upon the twin concepts of <strong>listeners</strong>
and <strong>sites</strong>. Listeners are the sources of replication data, while
sites are the destination of replication data. Sites and listeners are
manually configured on each node in a cluster. This can be a burden to
the administrator as clusters become larger.</li>
  <li>A single connection tied to the <strong>cluster leader</strong> manages all
replication communications. This can cause performance problems on the
leader and is a bottleneck for realtime and fullsync replication data.</li>
  <li>Connections are established from site to listener. This can be
confusing for firewall administrators.</li>
  <li>The realtime replication queue will be lost if the replication
connection breaks, even if it’s re-established. Reconciling data in
this situation would require manual intervention using either of the
following:
    <ul>
      <li>a fullsync</li>
      <li>another Riak write to the key/value on the listener, thus
  re-queueing the object</li>
    </ul>
  </li>
  <li>Riak CS MDC <code class="language-plaintext highlighter-rouge">proxy_get</code> connections can only request data from a
single leader node</li>
</ul>

<h3 id="when-to-use-version-2-replication">When to use version 2 replication</h3>

<ul>
  <li>If you are running clusters below version 1.3.0 of Riak Enterprise,
version 2 replication is the only method of replication available.</li>
  <li>In the Riak 1.3 series, version 3 replication was provided as a
technology preview and did not have feature parity with version 2.
This was provided in the Riak 1.4 series.</li>
</ul>

<h2 id="version-3">Version 3</h2>

<ul>
  <li>Version 3 replication uses the twin concepts of <strong>sources</strong> and
<strong>sinks</strong>. A source is considered the primary provider of replication
data, whereas a sink is the destination of replication data.</li>
  <li>Establishing replication connections between clusters has been
greatly simplified. A single <code class="language-plaintext highlighter-rouge">riak-repl connect</code> command needs to be
issued from a source cluster to a sink cluster. IP and port
information of all nodes that can participate in replication on both
source and sink clusters are exchanged by the <strong>replication cluster
manager</strong>. The replication cluster manager also tracks nodes joining
and leaving the cluster dynamically.</li>
  <li>If the source has M nodes, and the sink has N nodes, there will be M
realtime connections. Connections aren’t tied to a leader node as they
are with version 2 replication.</li>
  <li>Communications for realtime, fullsync, and <code class="language-plaintext highlighter-rouge">proxy_get</code> operations are
multiplexed over the same connection for each node participating in
replication. This reduces the amount of firewall configuration on both
sources and sinks.</li>
  <li>A fullsync coordinator runs on a leader of the source cluster. The
coordinator assigns work across nodes in the sources cluster in an
optimized fashion.</li>
  <li>Realtime replication establishes a bounded queue on each source node
that is shared between <em>all</em> sinks. This queue requires consumers to
acknowledge objects when they have been replicated. Dropped TCP
connections won’t drop objects from the queue.</li>
  <li>If a node in the source cluster is shut down via the command line, a
realtime replication queue is migrated to other running nodes in the
source cluster.</li>
  <li>Network statistics are kept per socket.</li>
  <li>Fullsyncs between clusters can be tuned to control the maximum number
of workers that will run on a source node, a sink node, and across the
entire source cluster. This allows for limiting impact on the cluster
and dialing in fullsync performance.</li>
  <li>Version 3 is able to take advantage of <a href="/riak/kv/2.2.3/learn/concepts/active-anti-entropy/">Active Anti-Entropy</a> (AAE)
technology, which can greatly improve fullsync performance.</li>
  <li>Riak CS MDC <code class="language-plaintext highlighter-rouge">proxy_get</code> connections will be distributed across the
source cluster (as CS blocks are requested from the sink cluster in
this scenario).</li>
</ul>
